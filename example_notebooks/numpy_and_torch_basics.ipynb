{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating Tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ranges of Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Create a 1D Numpy array containing the 32-bit floating point values from 0 to 11\n",
    "x_numpy = np.arange(12, dtype=np.float32)\n",
    "# Create a 1D Torch tensor containing the 32-bit floating point values from 0 to 11\n",
    "x_torch = torch.arange(12, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
      "(12,) torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "print(type(x_numpy), type(x_torch))\n",
    "print(x_numpy.shape, x_torch.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ones and Zeros"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Containing only ones with same data types and size as before\n",
    "y_numpy_ones = np.ones_like(x_numpy)\n",
    "y_torch_ones = torch.ones_like(x_torch)\n",
    "\n",
    "# Containing only zeros with same data types and size as before\n",
    "z_numpy_zeros = np.zeros_like(y_numpy_ones)\n",
    "z_torch_zeros = torch.zeros_like(y_torch_ones)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0 tensor(12.)\n",
      "0.0 tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(y_numpy_ones.sum(), y_torch_ones.sum())\n",
    "print(z_numpy_zeros.sum(), z_torch_zeros.sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "# Count the number of elements in the tensors/array. This is a case where the two libraries differ a bit\n",
    "print(y_torch_ones.numel())\n",
    "print(y_numpy_ones.size)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Let's look at creating some random values. Let's say we want a 2D tensor of shape 5x5 with random 16-bit floating points values samples from a Gaussian distribution\n",
    "r_numpy = np.random.randn(5, 5).astype(np.float16)\n",
    "r_torch = torch.randn((5,5), dtype=torch.float16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[-0.417  ,  0.754  , -1.606  , -0.6123 ,  1.52   ],\n       [ 2.244  , -1.29   , -0.05856, -0.4785 , -0.679  ],\n       [-0.5835 ,  2.047  , -0.1962 ,  0.1606 , -0.04477],\n       [-0.789  , -0.9805 , -0.5522 , -0.394  ,  0.8716 ],\n       [-0.0983 , -1.545  ,  0.0902 ,  1.551  ,  0.04935]], dtype=float16)"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_numpy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-2.0605,  0.1648,  1.2422,  0.5728, -1.2871],\n        [ 1.0342,  0.1401,  0.3108,  0.2815, -0.0604],\n        [-0.8438, -0.5664,  1.6270,  1.1523,  0.3770],\n        [ 0.0344,  0.6152,  1.2246, -0.0669, -0.1643],\n        [ 0.9082,  1.0449, -0.9507, -1.3271,  0.7451]], dtype=torch.float16)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reshaping Tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  1.,  2.,  3.],\n       [ 4.,  5.,  6.,  7.],\n       [ 8.,  9., 10., 11.]], dtype=float32)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_numpy.reshape(3,4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_torch.reshape(3,4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0.,  1.,  2.,  3.],\n       [ 4.,  5.,  6.,  7.],\n       [ 8.,  9., 10., 11.]], dtype=float32)"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also ask Numpy/Torch to automatically infer one of the dimensions\n",
    "x_numpy.reshape(3, -1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_torch.reshape(-1, 4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Indexing and Slicing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# Let's create tensors of shape 3,4,5, containing random integers between 0 and 10\n",
    "# The indexing and slicing mechanisms are the same for Numpy and Torch, so we will just use Torch for now\n",
    "t_torch = torch.randint(0, 10, (3,4,5), dtype=torch.int8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[3, 4, 9, 0, 3],\n         [5, 4, 0, 4, 5],\n         [2, 1, 4, 1, 8],\n         [5, 8, 7, 5, 0]],\n\n        [[6, 3, 6, 4, 2],\n         [0, 9, 3, 8, 3],\n         [5, 0, 2, 3, 9],\n         [4, 4, 4, 1, 3]],\n\n        [[5, 3, 9, 4, 7],\n         [1, 7, 9, 7, 2],\n         [9, 9, 2, 0, 8],\n         [9, 0, 2, 9, 3]]], dtype=torch.int8)"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grab just the first 4x5 tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[3, 4, 9, 0, 3],\n        [5, 4, 0, 4, 5],\n        [2, 1, 4, 1, 8],\n        [5, 8, 7, 5, 0]], dtype=torch.int8)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_torch[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grab the last row in the last 4x5 tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([9, 0, 2, 9, 3], dtype=torch.int8)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_torch[-1][-1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([9, 0, 2, 9, 3], dtype=torch.int8)"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Equivalent way of doing this with slicing\n",
    "t_torch[-1,-1,:]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grab just the first 2 columns in the first row of the second 4x5 tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([6, 3], dtype=torch.int8)"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_torch[1,0,0:2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grab the element in the 2nd row and 3rd column of the third 4x5 tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(9, dtype=torch.int8)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is equivalent to, but faster than t_torch[2][1][2]\n",
    "t_torch[2,1,2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Assigning Values\n",
    "### You can rewrite values of a tensor using the same indexing and slicing mechanisms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "my_tensor = torch.tensor([4, 4, 3, 3, 2, 2, 1, 1, 0], dtype=torch.float16)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "my_tensor[0] = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5., 4., 3., 3., 2., 2., 1., 1., 0.], dtype=torch.float16)"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "my_tensor[-2:] = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([5., 4., 3., 3., 2., 2., 1., 0., 0.], dtype=torch.float16)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "my_other_tensor = torch.tensor([[1, 2, 3], [3, 2, 1], [0, 1, 0], [0, 0, 0]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([4, 3])"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_other_tensor.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "my_other_tensor[1, 0:2] = torch.tensor([8, 9])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 2, 3],\n        [8, 9, 1],\n        [0, 1, 0],\n        [0, 0, 0]])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_other_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Combining Tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "tensor1 = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int8)\n",
    "tensor2 = torch.tensor([[7, 8 ,9], [10, 11, 12]], dtype=torch.int8)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]], dtype=torch.int8)"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can concatenate tensors together. To do so, you tell Torch on which axis you want to concatenate\n",
    "# let's first concatenate along rows\n",
    "tensor3 = torch.cat((tensor1, tensor2), dim=0)\n",
    "tensor3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1,  2,  3,  7,  8,  9],\n        [ 4,  5,  6, 10, 11, 12]], dtype=torch.int8)"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's concatenate along columns\n",
    "tensor4 = torch.cat((tensor1, tensor2), dim=1)\n",
    "tensor4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Unary Operators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 2., 4., 6., 8.], dtype=torch.float64)"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 2, 4, 6, 8], dtype=torch.float64)\n",
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 2., 4., 6., 8.], dtype=torch.float64)"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# absolute value\n",
    "x.abs()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0000, 1.4142, 2.0000, 2.4495, 2.8284], dtype=torch.float64)"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sqrt\n",
    "x.sqrt()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.0000e+00, 7.3891e+00, 5.4598e+01, 4.0343e+02, 2.9810e+03],\n       dtype=torch.float64)"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e^x\n",
    "x.exp()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Binary Operators"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 2, 4, 6, 8], dtype=torch.float64)\n",
    "y = torch.tensor([1, 3, 2, 1, 4], dtype=torch.float64)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.,  5.,  6.,  7., 12.], dtype=torch.float64)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise addition of two vectors\n",
    "x + y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([-1., -1.,  2.,  5.,  4.], dtype=torch.float64)"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise subtraction of two vectors\n",
    "x - y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.,  6.,  8.,  6., 32.], dtype=torch.float64)"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise multiplication of two vectors\n",
    "x * y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0.0000, 0.6667, 2.0000, 6.0000, 2.0000], dtype=torch.float64)"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise division of two vectors\n",
    "x / y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   0.,    8.,   16.,    6., 4096.], dtype=torch.float64)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elementwise exponentiation of two vectors\n",
    "x ** y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Broadcasting\n",
    "\n",
    "### Elementwise vector operations can sometimes be applied to tensors of differing shape, using broadcasting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "x = torch.tensor([[0], [1], [2]])\n",
    "y = torch.tensor([[0, 1]])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0],\n         [1],\n         [2]]),\n tensor([[0, 1]]))"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In the following operation, x will expand from 1 to 2 columns to result in a 3x2 tensor, copying column 0 of x into column 1. Likewise, y will expand from a 1x2 tensor to a 3x2 tensor by copying row 0 twice. The expanded forms of x and y will then be added"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1],\n        [1, 2],\n        [2, 3]])"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Memory Allocation\n",
    "\n",
    "### Operations on tensors can create additional memory overhead in some cases"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [],
   "source": [
    "x = torch.tensor([0,1,2])\n",
    "y = torch.tensor([0,1,2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "(5770442800, 5770439520)"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# id() gets the memory address\n",
    "id(x), id(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "y = x + y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "data": {
      "text/plain": "5766704448"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As we can see, reassigning y to x + y ends up storing the result in a different memory location."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "(6131474304, 6131482304)"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0,1,2])\n",
    "y = torch.tensor([0,1,2])\n",
    "id(x), id(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "outputs": [],
   "source": [
    "y += x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "data": {
      "text/plain": "6131482304"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using y += x, which is also equivalent to y[:] = y + x, perhaps the operation in-place"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Numpy/Torch Conversions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "outputs": [
    {
     "data": {
      "text/plain": "numpy.ndarray"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a torch tensor to a Numpy array\n",
    "y_numpy = y.numpy()\n",
    "type(y_numpy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "outputs": [],
   "source": [
    "# convert a Numpy array to a Torch tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Tensor"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_numpy = np.arange(3)\n",
    "x_torch = torch.from_numpy(x_numpy)\n",
    "type(x_torch)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tensor Operations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[2., 2., 2., 2.],\n         [2., 2., 2., 2.],\n         [2., 2., 2., 2.]]),\n tensor([[3., 3., 3., 3.],\n         [3., 3., 3., 3.],\n         [3., 3., 3., 3.]]))"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.ones((3,4)) * 2\n",
    "B = torch.ones((3,4)) * 3\n",
    "A,B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hadamard Product"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[6., 6., 6., 6.],\n        [6., 6., 6., 6.],\n        [6., 6., 6., 6.]])"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A * B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reduction Operations\n",
    "### These operations reduce the order of tensors along one or more axes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.) tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "# Reduction of nth order tensor to 1st order tensor\n",
    "# total sum and total mean\n",
    "print(A.sum(), A.mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([6., 6., 6., 6.]), tensor([2., 2., 2., 2.]))"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduction of nth order tensor to n-1'th order tensor\n",
    "# row sum and row mean\n",
    "A.sum(axis=0), A.mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([8., 8., 8.]), tensor([2., 2., 2.]))"
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column sum and column mean\n",
    "A.sum(axis=1), A.mean(axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-reducing Sum or Mean\n",
    "We can set keepdims=True to not get rid of the reduced axis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8., 8., 8.]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# reducing sum\n",
    "C = A.sum(axis=1)\n",
    "print(C, C.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.],\n",
      "        [8.],\n",
      "        [8.]]) torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# non-reducing sum\n",
    "C = A.sum(axis=1, keepdim=True)\n",
    "print(C, C.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Norms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [],
   "source": [
    "x = torch.tensor([0, 4, 1, 3, 8], dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## L2 Norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(9.4868)"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.norm()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## L1 Norm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(16.)"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.abs().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Euclidean distance between two vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(9.8995)"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 4, 1, 3, 8], dtype=torch.float32)\n",
    "y = torch.tensor([-8, 5, 2, -1, 4], dtype=torch.float32)\n",
    "\n",
    "torch.norm(x-y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vector Dot Product"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "u = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "v = torch.tensor([4, 1, -5], dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-9.)"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u@v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-9.)"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following is identical to above\n",
    "torch.dot(u, v)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Matrix Vector Product\n",
    "\n",
    "## Example of dimensionality reduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 3]), torch.Size([3]))"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 0, 1], [0, 1, -1]], dtype=torch.float32)\n",
    "x = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "A.shape, x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4., -1.])"
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 4., -1.])"
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following is identical to above\n",
    "torch.mv(A, x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example of 90 degree rotation operation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 2]), torch.Size([2]))"
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[0, -1], [1, 0]], dtype=torch.float32)\n",
    "x = torch.tensor([1, 0], dtype=torch.float32)\n",
    "A.shape, x.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0., 1.])"
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Matrix Matrix Product"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([2, 3]), torch.Size([3, 4]))"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "B = torch.tensor([[1, 1, 1, 1],[2, 2, 2, 2],[3, 3, 3, 3]], dtype=torch.float32)\n",
    "A.shape, B.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[14., 14., 14., 14.],\n        [32., 32., 32., 32.]])"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[14., 14., 14., 14.],\n        [32., 32., 32., 32.]])"
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the follow is identical to above\n",
    "torch.mm(A, B)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Autograd\n",
    "\n",
    "## Let's First Consider a Scalar-valued Function\n",
    "\n",
    "### $y = 2 \\mathbf{x}^\\top \\mathbf{x}$\n",
    "\n",
    "Let $\\mathbf{x}$ be a column vector in $\\mathbb{R}^n$. The function is defined as:\n",
    "$$\n",
    "y = 2 \\mathbf{x}^\\top \\mathbf{x}\n",
    "$$\n",
    "This is equivalent to:\n",
    "$$\n",
    "y = 2 \\sum_{i=1}^n x_i^2\n",
    "$$\n",
    "\n",
    "In PyTorch, we can compute this function and its gradient using **Autograd**.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of y with respect to x:\n",
      "tensor([[ 0.],\n",
      "        [ 4.],\n",
      "        [ 8.],\n",
      "        [12.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define x as a column vector with requires_grad=True to enable gradient computation\n",
    "x = torch.tensor([[0.0], [1.0], [2.0], [3.0]], requires_grad=True)\n",
    "\n",
    "# Define the function y = 2 * x^T * x\n",
    "y = 2 * torch.matmul(x.T, x)\n",
    "\n",
    "# Perform backpropagation\n",
    "y.backward()\n",
    "\n",
    "# Display the computed gradient\n",
    "print(\"Gradient of y with respect to x:\")\n",
    "print(x.grad)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.],\n        [1.],\n        [1.],\n        [1.]])"
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()  # Reset the gradient\n",
    "y = x.sum()  # Redefine a different function\n",
    "y.backward()  # backpropagate\n",
    "x.grad  # get the gradient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Let's Now Consider a Vector-valued Function\n",
    "\n",
    "Let:\n",
    "$$\n",
    "\\mathbf{u}(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "u_1(\\mathbf{x}) \\\\\n",
    "u_2(\\mathbf{x})\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "x_1^2 + x_2 \\\\\n",
    "\\sin(x_1) + x_2^3\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The Jacobian matrix is:\n",
    "$$\n",
    "J_{\\mathbf{u}}(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial u_1}{\\partial x_1} & \\frac{\\partial u_1}{\\partial x_2} \\\\\n",
    "\\frac{\\partial u_2}{\\partial x_1} & \\frac{\\partial u_2}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2x_1 & 1 \\\\\n",
    "\\cos(x_1) & 3x_2^2\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "At\n",
    "\n",
    "$$\\( \\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\)$$we have\n",
    "$$\n",
    "J_{\\mathbf{u}}(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "2(1) & 1 \\\\\n",
    "\\cos(1) & 3(2)^2\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "2 & 1 \\\\\n",
    "\\cos(1) & 12\n",
    "\\end{bmatrix}.\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian of u with respect to x:\n",
      "tensor([[ 2.0000,  1.0000],\n",
      "        [ 0.5403, 12.0000]], grad_fn=<StackBackward0>)\n",
      "Gradient of y with respect to x: tensor([ 5.6209, 38.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define input vector x with gradients enabled\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)  # x = [x1, x2]\n",
    "\n",
    "# Define the vector-valued function u(x)\n",
    "u1 = x[0]**2 + x[1]  # u1 = x1^2 + x2\n",
    "u2 = torch.sin(x[0]) + x[1]**3  # u2 = sin(x1) + x2^3\n",
    "u = torch.stack([u1, u2])  # Stack outputs into a vector u = [u1, u2]\n",
    "\n",
    "# Compute the Jacobian of u(x) with respect to x before computing the gradient of y\n",
    "jacobian = []\n",
    "for i in range(len(u)):\n",
    "    grad_u = torch.autograd.grad(u[i], x, retain_graph=True, create_graph=True)[0]\n",
    "    jacobian.append(grad_u)\n",
    "jacobian = torch.stack(jacobian)\n",
    "\n",
    "print(\"Jacobian of u with respect to x:\")\n",
    "print(jacobian)\n",
    "\n",
    "# Define a scalar function y(u)\n",
    "y = 2 * u[0] + 3 * u[1]  # y = 2*u1 + 3*u2\n",
    "\n",
    "# Perform backpropagation\n",
    "y.backward(retain_graph=True)  # Retain graph for subsequent operations\n",
    "\n",
    "# Print the gradient of y with respect to x\n",
    "print(\"Gradient of y with respect to x:\", x.grad)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
